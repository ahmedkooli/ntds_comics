{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pickle\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sys import stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Marvel dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Parse the Marvel characters\n",
    "\n",
    "**We go on the page where all the `Earth-616` Marvel characters are. We start with the first page and we make a list of all the URLs for the different characters.**\n",
    "\n",
    "**Collect the first URLs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://marvel.fandom.com/wiki/Category:Earth-616_Characters'\n",
    "r = requests.get(URL)\n",
    "page_body = r.text\n",
    "soup = BeautifulSoup(page_body, 'html.parser')\n",
    "\n",
    "# Collect url of each character in the web page\n",
    "publications_wrappers = soup.find_all('li', class_='category-page__member')\n",
    "list_url = []\n",
    "\n",
    "for p in publications_wrappers:\n",
    "    for a in p.find_all('a', href=True):\n",
    "        list_url.append(a['href'])\n",
    "        \n",
    "# Remove duplicate\n",
    "my_set = set(list_url)\n",
    "good_list_url = list(my_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We then use these URLs to collect the needed characterstics for each Marvel character.**\n",
    "\n",
    "**Parse the first page:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "new_columns = ['URL', 'Real Name', 'Current Alias', 'Relatives', 'Affiliation']\n",
    "\n",
    "personnage_pd = pd.DataFrame(columns=new_columns)\n",
    "idx = 0\n",
    "dict_geant={}\n",
    "\n",
    "for pers in good_list_url:\n",
    "    # Get URL and use html parser\n",
    "    URL_char = 'https://marvel.fandom.com' + pers\n",
    "    URL_char = URL_char.replace(\"'\",\"\")\n",
    "    r_char = requests.get(URL_char)\n",
    "    page_body_char = r_char.text\n",
    "    soup_char = BeautifulSoup(page_body_char, 'html.parser')\n",
    "    \n",
    "    # Initialize variables\n",
    "    url, name, current_alias, relatives, affiliation = '','','','',''\n",
    "    personnage =[]\n",
    "    \n",
    "    # Parsing\n",
    "    url = pers\n",
    "    side_tab = soup_char.find_all('div', class_='conjoined-infoboxes')\n",
    "    for p in side_tab:\n",
    "        for pp in p.find_all('div', class_='pi-item pi-data pi-item-spacing pi-border-color'):\n",
    "            for ppp in pp.find_all('h3', class_='pi-data-label pi-secondary-font'):\n",
    "                if(ppp.text[1:]==\"Real Name\"):\n",
    "                    name = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[1:]==\"Current Alias\"):\n",
    "                    current_alias = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[1:]==\"Relatives\"):\n",
    "                    div = pp.find('div', class_='pi-data-value pi-font')\n",
    "                    if div:\n",
    "                        for a in div.find_all('a', href=True):\n",
    "                            relatives = relatives + ', ' + a['href']\n",
    "                if(ppp.text[1:]==\"Affiliation\"):\n",
    "                    div = pp.find('div', class_='pi-data-value pi-font')\n",
    "                    if div:\n",
    "                        for a in div.find_all('a', href=True):\n",
    "                            affiliation = affiliation + ', ' + a['href']\n",
    "\n",
    "    characteristics_pd = pd.DataFrame([[url, name[1:], current_alias, relatives, affiliation]], columns = new_columns)\n",
    "    personnage_pd = personnage_pd.append(characteristics_pd,ignore_index=True)\n",
    "    \n",
    "nextpage = soup.find('link', {\"rel\" : \"next\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After parsing the first page, we can get the links for the next pages, parsing them one by one. After that, we use the collected links to access each character's webpage exactly as we did for the first page. We then follow by storing the characterstics we need for our analysis.**\n",
    "\n",
    "**Parse the remaining pages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.578325 %"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-a47a013aca26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtot_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m27839\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnextpage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprinted\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtot_page\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "tot_page = 27839/200\n",
    "\n",
    "while(len(nextpage['href'])):\n",
    "    i += 1\n",
    "    printed= i/tot_page*100\n",
    "    stdout.write(\"\\r%f %%\" % printed)\n",
    "    stdout.flush()\n",
    "    \n",
    "    urlnext_page = nextpage['href']\n",
    "    r = requests.get(urlnext_page)\n",
    "    page_body = r.text\n",
    "    soup = BeautifulSoup(page_body, 'html.parser')\n",
    "\n",
    "    # Collect URL of each character in the web page\n",
    "    publications_wrappers = soup.find_all('li', class_='category-page__member')\n",
    "    list_url = []\n",
    "    for p in publications_wrappers:\n",
    "        for a in p.find_all('a', href=True):\n",
    "            list_url.append(a['href'])\n",
    "            \n",
    "    # Remove duplicate\n",
    "    my_set = set(list_url)\n",
    "    good_list_url = list(my_set)\n",
    "    \n",
    "    idx = 0\n",
    "    dict_geant = {}\n",
    "    \n",
    "    for pers in good_list_url:\n",
    "        # Get URL and use html parser\n",
    "        URL_char = 'https://marvel.fandom.com' + pers\n",
    "        URL_char = URL_char.replace(\"'\",\"\")\n",
    "        r_char = requests.get(URL_char)\n",
    "        page_body_char = r_char.text\n",
    "        soup_char = BeautifulSoup(page_body_char, 'html.parser')\n",
    "        \n",
    "        # Initialize variables\n",
    "        url, name, current_alias, relatives, affiliation = '','','','',''\n",
    "        personnage =[]\n",
    "        \n",
    "        # Parsing\n",
    "        side_tab = soup_char.find_all('div', class_='conjoined-infoboxes')\n",
    "        url = pers\n",
    "        for p in side_tab:\n",
    "            for pp in p.find_all('div', class_='pi-item pi-data pi-item-spacing pi-border-color'):\n",
    "                for ppp in pp.find_all('h3', class_='pi-data-label pi-secondary-font'):\n",
    "                    if(ppp.text[1:]==\"Real Name\"):\n",
    "                        name = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                    if(ppp.text[1:]==\"Current Alias\"):\n",
    "                        current_alias = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                    if(ppp.text[1:]==\"Relatives\"):\n",
    "                        div = pp.find('div', class_='pi-data-value pi-font')\n",
    "                        if div:\n",
    "                            for a in div.find_all('a', href=True):\n",
    "                                relatives = relatives + ', ' + a['href']\n",
    "                    if(ppp.text[1:]==\"Affiliation\"):\n",
    "                        div = pp.find('div', class_='pi-data-value pi-font')\n",
    "                        if div:\n",
    "                            for a in div.find_all('a', href=True):\n",
    "                                affiliation = affiliation + ', ' + a['href']\n",
    "\n",
    "        characteristics_pd = pd.DataFrame([[url, name[1:], current_alias, relatives, affiliation]], columns = new_columns)\n",
    "        personnage_pd = personnage_pd.append(characteristics_pd,ignore_index=True)\n",
    "\n",
    "    nextpage = soup.find('link', {\"rel\" : \"next\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since the parsing takes a relatively high amount of time, it's better to save it to pickle.**\n",
    "\n",
    "**Save file to pickle:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(personnage_pd, open('data/characters_marvel.txt','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open the saved file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Real Name</th>\n",
       "      <th>Current Alias</th>\n",
       "      <th>Relatives</th>\n",
       "      <th>Affiliation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Aaron_Fox_(Earth-616)</td>\n",
       "      <td>Aaron Fox</td>\n",
       "      <td></td>\n",
       "      <td>, /wiki/Beth_Fox_(Earth-616)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>/wiki/Acrobat_(1940s)_(Earth-616)</td>\n",
       "      <td>nknown</td>\n",
       "      <td>Acrobat</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>/wiki/Abigail_Mercury_(Clone)_(Earth-616)</td>\n",
       "      <td>Abigail Mercury</td>\n",
       "      <td></td>\n",
       "      <td>, /wiki/Abigail_Mercury_(Earth-616)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>/wiki/Ace_Maxwell_(Earth-616)</td>\n",
       "      <td>Ace Maxwell</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>/wiki/Abigail_Boylen_(Earth-616)</td>\n",
       "      <td>Abigail \"Abby\" Boylen</td>\n",
       "      <td>Cloud 9</td>\n",
       "      <td>, #cite_note-Avengers_The_Initiative_Vol_1_1-2</td>\n",
       "      <td>, /wiki/Champions_(Earth-616), /wiki/Undergrou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28034</td>\n",
       "      <td>/wiki/Zxaxz_(Earth-616)</td>\n",
       "      <td>Zxaxz</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28035</td>\n",
       "      <td>/wiki/Zuwena_(Earth-616)</td>\n",
       "      <td>Zuwena</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>, /wiki/Elephant%27s_Trunk_(Earth-616)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28036</td>\n",
       "      <td>/wiki/Zurvan_(Earth-616)</td>\n",
       "      <td>Zurvan</td>\n",
       "      <td></td>\n",
       "      <td>, /wiki/Ahura_Mazda_(Earth-616), /wiki/Ahriman...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28037</td>\n",
       "      <td>/wiki/Zygo_(Earth-616)</td>\n",
       "      <td>Zygo</td>\n",
       "      <td>General Zygo</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28038</td>\n",
       "      <td>/wiki/%C3%84kr%C3%A4s_(Earth-616)</td>\n",
       "      <td>Äkräs[1]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>, /wiki/Jumala</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28039 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             URL              Real Name  \\\n",
       "0                    /wiki/Aaron_Fox_(Earth-616)              Aaron Fox   \n",
       "1              /wiki/Acrobat_(1940s)_(Earth-616)                 nknown   \n",
       "2      /wiki/Abigail_Mercury_(Clone)_(Earth-616)        Abigail Mercury   \n",
       "3                  /wiki/Ace_Maxwell_(Earth-616)            Ace Maxwell   \n",
       "4               /wiki/Abigail_Boylen_(Earth-616)  Abigail \"Abby\" Boylen   \n",
       "...                                          ...                    ...   \n",
       "28034                    /wiki/Zxaxz_(Earth-616)                  Zxaxz   \n",
       "28035                   /wiki/Zuwena_(Earth-616)                 Zuwena   \n",
       "28036                   /wiki/Zurvan_(Earth-616)                 Zurvan   \n",
       "28037                     /wiki/Zygo_(Earth-616)                   Zygo   \n",
       "28038          /wiki/%C3%84kr%C3%A4s_(Earth-616)               Äkräs[1]   \n",
       "\n",
       "      Current Alias                                          Relatives  \\\n",
       "0                                         , /wiki/Beth_Fox_(Earth-616)   \n",
       "1           Acrobat                                                      \n",
       "2                                  , /wiki/Abigail_Mercury_(Earth-616)   \n",
       "3                                                                        \n",
       "4           Cloud 9     , #cite_note-Avengers_The_Initiative_Vol_1_1-2   \n",
       "...             ...                                                ...   \n",
       "28034                                                                    \n",
       "28035                                                                    \n",
       "28036                , /wiki/Ahura_Mazda_(Earth-616), /wiki/Ahriman...   \n",
       "28037  General Zygo                                                      \n",
       "28038                                                                    \n",
       "\n",
       "                                             Affiliation  \n",
       "0                                                         \n",
       "1                                                         \n",
       "2                                                         \n",
       "3                                                         \n",
       "4      , /wiki/Champions_(Earth-616), /wiki/Undergrou...  \n",
       "...                                                  ...  \n",
       "28034                                                     \n",
       "28035             , /wiki/Elephant%27s_Trunk_(Earth-616)  \n",
       "28036                                                     \n",
       "28037                                                     \n",
       "28038                                     , /wiki/Jumala  \n",
       "\n",
       "[28039 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/characters_marvel.txt', 'rb') as f:\n",
    "    characters_marvel = pickle.load(f)\n",
    "\n",
    "characters_marvel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Parse the Marvel comics\n",
    "\n",
    "**Similarly to the Marvel characters, we want to first collect the comics' URLs to get to a specific comic's page. We start by the first page.**\n",
    "\n",
    "**Collect the first URLs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get URL and use html parser\n",
    "URL = 'https://marvel.fandom.com/wiki/Category:Comics'\n",
    "r = requests.get(URL)\n",
    "page_body = r.text\n",
    "soup = BeautifulSoup(page_body, 'html.parser')\n",
    "\n",
    "# Collect URL of each character in the web page\n",
    "publications_wrappers = soup.find_all('li', class_='category-page__member')\n",
    "\n",
    "# Initialization\n",
    "i = 0\n",
    "list_url = []\n",
    "\n",
    "for p in publications_wrappers:\n",
    "    # Don't take into account the first links which are \"Categories\"\n",
    "    if(i>=26):\n",
    "        for a in p.find_all('a', href=True):\n",
    "            list_url.append(a['href'])\n",
    "    i +=1\n",
    "    \n",
    "list_url[:20]\n",
    "\n",
    "# Remove duplicate\n",
    "my_set = set(list_url)\n",
    "good_list_url = list(my_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After getting the URLs, we parse all the information we need from each page. Let's note that a comic can have more than one story that are separate. We will name them `subcomic`. They will be treated as different comics since the appearing characters and the writers are often different.**\n",
    "\n",
    "**Parse the first page:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comics_columns = ['URL', 'Good characters', 'Bad characters', \n",
    "                  'Neutral characters', 'Editor-in-chief', 'Editor-in-chief URL', \n",
    "                  'Writer', 'Writer URL', 'Publication date', 'Subcomic']\n",
    "\n",
    "comics_pd = pd.DataFrame(columns=comics_columns)\n",
    "\n",
    "\n",
    "# Parse the first page\n",
    "for comics in good_list_url:\n",
    "    # Get URL and use html parser\n",
    "    URL_char = 'https://marvel.fandom.com' + comics\n",
    "    URL_char = URL_char.replace(\"'\",\"\")\n",
    "    r_char = requests.get(URL_char)\n",
    "    page_body_char = r_char.text\n",
    "    soup_char = BeautifulSoup(page_body_char, 'html.parser')\n",
    "    \n",
    "    # Initialization\n",
    "    good, bad, neutral = '','',''\n",
    "    editor, writer, publication = '','',''\n",
    "    editorURL, writerURL, subcomic = '','',''\n",
    "    \n",
    "    URL2 = URL_char.replace('https://marvel.fandom.com','')\n",
    "\n",
    "    # Boolean for the first case\n",
    "    first = 1\n",
    "\n",
    "    # Parse characters appearances\n",
    "    appearances = soup_char.find_all('div', class_='mw-content-ltr mw-content-text')\n",
    "\n",
    "    for p in appearances:\n",
    "        for pp in p.find_all('p'):\n",
    "            # If a comic is split in sub-comics\n",
    "            span = pp.find_previous('span', class_='mw-headline')\n",
    "            \n",
    "            if span and (\"Appearing\" in span.text):\n",
    "                if (not first and subcomic!=span.text \\\n",
    "                and (good or bad or neutral)):\n",
    "                    # Take only the title of the subcomic\n",
    "                    subcomic = subcomic[14:-1]\n",
    "                    appearances_pd = pd.DataFrame([[URL2, good, bad, neutral, \n",
    "                                        editor, editorURL,\n",
    "                                        writer, writerURL,\n",
    "                                        publication, subcomic]], columns = comics_columns)\n",
    "                    comics_pd = comics_pd.append(appearances_pd)\n",
    "\n",
    "                    good, bad, neutral = '','',''\n",
    "                    editor, writer, publication = '','',''\n",
    "                    editorURL, writerURL = '',''\n",
    "                    subcomic = span.text\n",
    "\n",
    "                else:\n",
    "                    first = 0\n",
    "                    subcomic = span.text\n",
    "\n",
    "            if \"Featured Characters:\" in pp.text:\n",
    "                ul = pp.find_next('ul')\n",
    "                for a in ul.find_all('a', href=True):\n",
    "                    good = good + ', ' + a['href']\n",
    "\n",
    "            if \"Supporting Characters:\" in pp.text:\n",
    "                ul = pp.find_next('ul')\n",
    "                for a in ul.find_all('a', href=True):\n",
    "                    good = good + ', ' + a['href']\n",
    "\n",
    "            if \"Antagonists:\" in pp.text:\n",
    "                ul = pp.find_next('ul')\n",
    "                for a in ul.find_all('a', href=True):\n",
    "                    bad = bad + ', ' + a['href']\n",
    "\n",
    "            if \"Other Characters:\" in pp.text:\n",
    "                ul = pp.find_next('ul')\n",
    "                for a in ul.find_all('a', href=True):\n",
    "                    neutral = neutral + ', ' + a['href']\n",
    "\n",
    "    # Each subcomic is written in the format '\"Appearing in *subcomic*\"'\n",
    "    # We keep only the subcomic name\n",
    "    subcomic = subcomic[14:-1]\n",
    "    appearances_pd = pd.DataFrame([[URL2, good, bad, neutral, \n",
    "                                    editor, editorURL,\n",
    "                                    writer, writerURL,\n",
    "                                    publication, subcomic]], columns = comics_columns)\n",
    "\n",
    "    comics_pd = comics_pd.append(appearances_pd)\n",
    "\n",
    "    subcomic = ''\n",
    "\n",
    "\n",
    "    # Parse comic characteristics\n",
    "    side_tab = soup_char.find_all('div', class_='infobox')\n",
    "\n",
    "    for p in side_tab:\n",
    "        # Publication date\n",
    "        tab = p.find_next('table', style='width:100%; text-align: center;')\n",
    "        if(tab):\n",
    "            tr = tab.find_next('tr', style='font-size:12px;')\n",
    "            td = tab.find_next('td', style='font-size:12px;')\n",
    "        if (tr):\n",
    "            publication = tr.find_next('td').text\n",
    "        if (td):\n",
    "            publication = td.text\n",
    "        if (not tr and not td):\n",
    "            publication = ''\n",
    "\n",
    "        # Editors\n",
    "        for pp in p.find_all('div', style='width:100px;float:left;text-align:left;'):\n",
    "            if ('Editor-in-Chief' in pp.text):\n",
    "                ppp = pp.find_next('div', style='width:190px;float:left;text-align:right;')\n",
    "                for a in ppp.find_all('a'):\n",
    "                    editor = editor + ', ' + a.text\n",
    "                for a in ppp.find_all('a', href=True):\n",
    "                    editorURL = editorURL + ', ' + a['href']\n",
    "                    \n",
    "        if(p.find_next('div', style='width:88%; text-align:left;')):\n",
    "            sub = p.find_next('div', style='width:88%; text-align:left;')\n",
    "\n",
    "        # Writers\n",
    "        for pp in p.find_all('div', style='width:100px;float:left;text-align:left;'):\n",
    "            if (pp.find_previous('div', style='width:88%; text-align:left;')):\n",
    "                sub = pp.find_previous('div', style='width:88%; text-align:left;')\n",
    "            if sub:\n",
    "                adiv = sub.find_next('div', style='width:100px;float:left;text-align:left;')\n",
    "\n",
    "            if (adiv and 'Writer' in adiv.text and 'Writer' in pp.text):\n",
    "                if (sub):\n",
    "                    subcomic = sub.text[1:-1]\n",
    "\n",
    "                ppp = pp.find_next('div', style='width:190px;float:left;text-align:right;')\n",
    "                for a in ppp.find_all('a'):\n",
    "                    writer = writer + ', ' + a.text\n",
    "                for a in ppp.find_all('a', href=True):\n",
    "                    writerURL = writerURL + ', ' + a['href']\n",
    "\n",
    "                comics_pd.loc[comics_pd.Subcomic == subcomic,'Writer'] = writer\n",
    "                comics_pd.loc[comics_pd.Subcomic == subcomic,'Writer URL'] = writerURL\n",
    "\n",
    "                if (sub):\n",
    "                    sub = sub.find_next('div', style='width:88%; text-align:left;')\n",
    "                    writer, writerURL = '',''\n",
    "\n",
    "            elif (sub): \n",
    "                sub = sub.find_next('div', style='width:88%; text-align:left;')\n",
    "                writer, writerURL = '',''    \n",
    "    \n",
    "    comics_pd.loc[comics_pd.URL == URL2,'Editor-in-chief'] = editor\n",
    "    comics_pd.loc[comics_pd.URL == URL2,'Editor-in-chief URL'] = editorURL\n",
    "    comics_pd.loc[comics_pd.URL == URL2,'Publication date'] = publication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Again, we use the next link to parse the remaining pages. The schema is exactly the same as the previous one.**\n",
    "\n",
    "**Parse the remaining pages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all the other pages\n",
    "nextpage = soup.find('link', {\"rel\" : \"next\"})\n",
    "\n",
    "i = 0\n",
    "tot_page = 49759/200\n",
    "\n",
    "while(len(nextpage['href'])):\n",
    "    # Print a loading bar\n",
    "    i += 1\n",
    "    printed= i/tot_page*100\n",
    "    stdout.write(\"\\r%f %%\" % printed)\n",
    "    stdout.flush()\n",
    "    urlnext_page = nextpage['href']\n",
    "    r = requests.get(urlnext_page)\n",
    "    page_body = r.text\n",
    "    soup = BeautifulSoup(page_body, 'html.parser')\n",
    "\n",
    "    # Collect url of each character in the web page\n",
    "    publications_wrappers = soup.find_all('li', class_='category-page__member')\n",
    "    list_url = []\n",
    "    for p in publications_wrappers:\n",
    "        for a in p.find_all('a', href=True):\n",
    "            list_url.append(a['href'])\n",
    "            \n",
    "    # Remove duplicates\n",
    "    my_set = set(list_url)\n",
    "    good_list_url = list(my_set)\n",
    "    \n",
    "    for comics in good_list_url:\n",
    "        # Get URL and use html parser\n",
    "        URL_char = 'https://marvel.fandom.com' + comics\n",
    "        URL_char = URL_char.replace(\"'\",\"\")\n",
    "        r_char = requests.get(URL_char)\n",
    "        page_body_char = r_char.text\n",
    "        soup_char = BeautifulSoup(page_body_char, 'html.parser')\n",
    "\n",
    "        # Initialization\n",
    "        good, bad, neutral = '','',''\n",
    "        editor, writer, publication = '','',''\n",
    "        editorURL, writerURL, subcomic = '','',''\n",
    "\n",
    "        URL2 = URL_char.replace('https://marvel.fandom.com','')\n",
    "\n",
    "        # Boolean for the first case\n",
    "        first = 1\n",
    "\n",
    "        # Parse characters appearances\n",
    "        appearances = soup_char.find_all('div', class_='mw-content-ltr mw-content-text')\n",
    "\n",
    "        for p in appearances:\n",
    "            for pp in p.find_all('p'):\n",
    "                # If a comic is split in sub-comics\n",
    "                span = pp.find_previous('span', class_='mw-headline')\n",
    "                \n",
    "                if span and (\"Appearing\" in span.text):\n",
    "                    if (not first and subcomic!=span.text \\\n",
    "                    and (good or bad or neutral)):\n",
    "                        # Take only the title of the subcomic\n",
    "                        subcomic = subcomic[14:-1]\n",
    "                        appearances_pd = pd.DataFrame([[URL2, good, bad, neutral, \n",
    "                                            editor, editorURL,\n",
    "                                            writer, writerURL,\n",
    "                                            publication, subcomic]], columns = comics_columns)\n",
    "                        comics_pd = comics_pd.append(appearances_pd)\n",
    "\n",
    "                        good, bad, neutral = '','',''\n",
    "                        editor, writer, publication = '','',''\n",
    "                        editorURL, writerURL = '',''\n",
    "                        subcomic = span.text\n",
    "\n",
    "                    else:\n",
    "                        first = 0\n",
    "                        subcomic = span.text\n",
    "\n",
    "                if \"Featured Characters:\" in pp.text:\n",
    "                    ul = pp.find_next('ul')\n",
    "                    for a in ul.find_all('a', href=True):\n",
    "                        good = good + ', ' + a['href']\n",
    "\n",
    "                if \"Supporting Characters:\" in pp.text:\n",
    "                    ul = pp.find_next('ul')\n",
    "                    for a in ul.find_all('a', href=True):\n",
    "                        good = good + ', ' + a['href']\n",
    "\n",
    "                if \"Antagonists:\" in pp.text:\n",
    "                    ul = pp.find_next('ul')\n",
    "                    for a in ul.find_all('a', href=True):\n",
    "                        bad = bad + ', ' + a['href']\n",
    "\n",
    "                if \"Other Characters:\" in pp.text:\n",
    "                    ul = pp.find_next('ul')\n",
    "                    for a in ul.find_all('a', href=True):\n",
    "                        neutral = neutral + ', ' + a['href']\n",
    "\n",
    "        # Each subcomic is written in the format '\"Appearing in *subcomic*\"'\n",
    "        # We keep only the subcomic name\n",
    "        subcomic = subcomic[14:-1]\n",
    "        appearances_pd = pd.DataFrame([[URL2, good, bad, neutral, \n",
    "                                        editor, editorURL,\n",
    "                                        writer, writerURL,\n",
    "                                        publication, subcomic]], columns = comics_columns)\n",
    "\n",
    "        comics_pd = comics_pd.append(appearances_pd)\n",
    "\n",
    "        subcomic = ''\n",
    "\n",
    "\n",
    "        # Parse comic characteristics\n",
    "        side_tab = soup_char.find_all('div', class_='infobox')\n",
    "\n",
    "        for p in side_tab:\n",
    "            # Publication date\n",
    "            if(p.find_next('div', style='width:88%; text-align:left;')):\n",
    "                tab = p.find_next('table', style='width:100%; text-align: center;')\n",
    "            if(tab):\n",
    "                tr = tab.find_next('tr', style='font-size:12px;')\n",
    "                td = tab.find_next('td', style='font-size:12px;')\n",
    "            if (tr):\n",
    "                publication = tr.find_next('td').text\n",
    "            if (td):\n",
    "                publication = td.text\n",
    "            if (not tr and not td):\n",
    "                publication = ''\n",
    "\n",
    "            # Editors\n",
    "            for pp in p.find_all('div', style='width:100px;float:left;text-align:left;'):\n",
    "                if ('Editor-in-Chief' in pp.text):\n",
    "                    ppp = pp.find_next('div', style='width:190px;float:left;text-align:right;')\n",
    "                    for a in ppp.find_all('a'):\n",
    "                        editor = editor + ', ' + a.text\n",
    "                    for a in ppp.find_all('a', href=True):\n",
    "                        editorURL = editorURL + ', ' + a['href']\n",
    "\n",
    "            sub = p.find_next('div', style='width:88%; text-align:left;')\n",
    "\n",
    "            # Writers\n",
    "            for pp in p.find_all('div', style='width:100px;float:left;text-align:left;'):\n",
    "                if (pp.find_previous('div', style='width:88%; text-align:left;')):\n",
    "                    sub = pp.find_previous('div', style='width:88%; text-align:left;')\n",
    "                if sub:\n",
    "                    adiv = sub.find_next('div', style='width:100px;float:left;text-align:left;')\n",
    "\n",
    "                if (adiv and 'Writer' in adiv.text and 'Writer' in pp.text):\n",
    "                    if (sub):\n",
    "                        subcomic = sub.text[1:-1]\n",
    "\n",
    "                    ppp = pp.find_next('div', style='width:190px;float:left;text-align:right;')\n",
    "                    for a in ppp.find_all('a'):\n",
    "                        writer = writer + ', ' + a.text\n",
    "                    for a in ppp.find_all('a', href=True):\n",
    "                        writerURL = writerURL + ', ' + a['href']\n",
    "\n",
    "                    comics_pd.loc[comics_pd.Subcomic == subcomic,'Writer'] = writer\n",
    "                    comics_pd.loc[comics_pd.Subcomic == subcomic,'Writer URL'] = writerURL\n",
    "\n",
    "                    if (sub):\n",
    "                        sub = sub.find_next('div', style='width:88%; text-align:left;')\n",
    "                        writer, writerURL = '',''\n",
    "\n",
    "                elif (sub): \n",
    "                    sub = sub.find_next('div', style='width:88%; text-align:left;')\n",
    "                    writer, writerURL = '',''    \n",
    "\n",
    "        comics_pd.loc[comics_pd.URL == URL2,'Editor-in-chief'] = editor\n",
    "        comics_pd.loc[comics_pd.URL == URL2,'Editor-in-chief URL'] = editorURL\n",
    "        comics_pd.loc[comics_pd.URL == URL2,'Publication date'] = publication\n",
    "    nextpage = soup.find('link', {\"rel\" : \"next\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save file to pickle:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(comics_pd, open('data/comics_marvel.txt','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open the saved file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Good characters</th>\n",
       "      <th>Bad characters</th>\n",
       "      <th>Neutral characters</th>\n",
       "      <th>Editor-in-chief</th>\n",
       "      <th>Editor-in-chief URL</th>\n",
       "      <th>Writer</th>\n",
       "      <th>Writer URL</th>\n",
       "      <th>Publication date</th>\n",
       "      <th>Subcomic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Marvel_Mystery_Comics_Vol_1_NN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>, Joe Caramagna</td>\n",
       "      <td>, /wiki/Joe_Caramagna</td>\n",
       "      <td>January, 1943</td>\n",
       "      <td>st stor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Comedy_Comics_Vol_1_12</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>, Stan Lee</td>\n",
       "      <td>, /wiki/Stan_Lee</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>December, 1942</td>\n",
       "      <td>Morphy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Marvel_Mystery_Comics_Vol_1_7</td>\n",
       "      <td>, /wiki/Human_Torch_(Android)_(Earth-616), /wi...</td>\n",
       "      <td>, /wiki/Roglo_(Earth-616), #cite_note-Only_App...</td>\n",
       "      <td>, /wiki/New_York_City_Police_Department_(Earth...</td>\n",
       "      <td>, Joe Simon</td>\n",
       "      <td>, /wiki/Joe_Simon</td>\n",
       "      <td>, Stan Lee, Larry Lieber</td>\n",
       "      <td>, /wiki/Stan_Lee, /wiki/Larry_Lieber</td>\n",
       "      <td>May, 1940</td>\n",
       "      <td>The Human Torch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Marvel_Mystery_Comics_Vol_1_7</td>\n",
       "      <td>, /wiki/Thomas_Halloway_(Earth-616), /wiki/Bet...</td>\n",
       "      <td>, /wiki/Emma_Martin_(Earth-616)</td>\n",
       "      <td>, /wiki/Henry_Martin_(Earth-616)</td>\n",
       "      <td>, Joe Simon</td>\n",
       "      <td>, /wiki/Joe_Simon</td>\n",
       "      <td>, Paul Gustavson, Ray Gill</td>\n",
       "      <td>, /wiki/Paul_Gustavson, /wiki/Ray_Gill</td>\n",
       "      <td>May, 1940</td>\n",
       "      <td>The Angel: Master of Men</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Marvel_Mystery_Comics_Vol_1_7</td>\n",
       "      <td>, /wiki/Namor_McKenzie_(Earth-616), /wiki/Thak...</td>\n",
       "      <td></td>\n",
       "      <td>, /wiki/Homo_mermanus, /wiki/New_York_City_Pol...</td>\n",
       "      <td>, Joe Simon</td>\n",
       "      <td>, /wiki/Joe_Simon</td>\n",
       "      <td>, William Blake Everett</td>\n",
       "      <td>, /wiki/William_Blake_Everett</td>\n",
       "      <td>May, 1940</td>\n",
       "      <td>Prince Namor, the Sub-Mariner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Spider-Man:_The_Complete_Clone_Saga_Epic...</td>\n",
       "      <td>, /wiki/Peter_Parker_(Earth-616), /wiki/Ben_Re...</td>\n",
       "      <td>, /wiki/Kaine_Parker_(Earth-616), /wiki/Samuel...</td>\n",
       "      <td>, /wiki/Guardian_(Spider-Clone)_(Earth-616), /...</td>\n",
       "      <td>, Joe Quesada</td>\n",
       "      <td>, /wiki/Joe_Quesada</td>\n",
       "      <td>, J.M. DeMatteis</td>\n",
       "      <td>, /wiki/J.M._DeMatteis</td>\n",
       "      <td>1979</td>\n",
       "      <td>Resurrection!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Spider-Man:_The_Complete_Clone_Saga_Epic...</td>\n",
       "      <td>, /wiki/Peter_Parker_(Earth-616), /wiki/Ben_Re...</td>\n",
       "      <td>, /wiki/Miles_Warren_(Jackal_Clone_2)_(Earth-616)</td>\n",
       "      <td>, /wiki/Kaine_Parker_(Earth-616), /wiki/Charle...</td>\n",
       "      <td>, Joe Quesada</td>\n",
       "      <td>, /wiki/Joe_Quesada</td>\n",
       "      <td>, Howard Mackie</td>\n",
       "      <td>, /wiki/Howard_Mackie</td>\n",
       "      <td>1979</td>\n",
       "      <td>Truths &amp; Deceptions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Hellraiser_Vol_1_17</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>, Clive Barker</td>\n",
       "      <td>, /wiki/Clive_Barker</td>\n",
       "      <td>1992</td>\n",
       "      <td>Resurrection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Ultimate_Spider-Man_Infinite_Comic_Vol_2_10</td>\n",
       "      <td>, /wiki/Peter_Parker_(Earth-12041), /wiki/Pete...</td>\n",
       "      <td>, /wiki/Shazana_(Earth-12041)</td>\n",
       "      <td>, /wiki/William_Howard_Taft_(Earth-12041), /wi...</td>\n",
       "      <td>, Axel Alonso</td>\n",
       "      <td>, /wiki/Axel_Alonso</td>\n",
       "      <td>, John Barber</td>\n",
       "      <td>, /wiki/John_Barber</td>\n",
       "      <td>2016</td>\n",
       "      <td>Ham-ilton (Part 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Marvel_Universe_Ultimate_Spider-Man_vs._...</td>\n",
       "      <td>, /wiki/Web-Warriors_(Earth-12041), /wiki/Pete...</td>\n",
       "      <td>, /wiki/Norman_Osborn_(Earth-TRN457), /wiki/Hy...</td>\n",
       "      <td>, /wiki/Rio_Morales_(Earth-TRN457), /wiki/Jeff...</td>\n",
       "      <td>, Axel Alonso</td>\n",
       "      <td>, /wiki/Axel_Alonso</td>\n",
       "      <td>, Joe Caramagna</td>\n",
       "      <td>, /wiki/Joe_Caramagna</td>\n",
       "      <td>2016</td>\n",
       "      <td>st stor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68482 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  URL  \\\n",
       "0                /wiki/Marvel_Mystery_Comics_Vol_1_NN   \n",
       "0                        /wiki/Comedy_Comics_Vol_1_12   \n",
       "0                 /wiki/Marvel_Mystery_Comics_Vol_1_7   \n",
       "0                 /wiki/Marvel_Mystery_Comics_Vol_1_7   \n",
       "0                 /wiki/Marvel_Mystery_Comics_Vol_1_7   \n",
       "..                                                ...   \n",
       "0   /wiki/Spider-Man:_The_Complete_Clone_Saga_Epic...   \n",
       "0   /wiki/Spider-Man:_The_Complete_Clone_Saga_Epic...   \n",
       "0                           /wiki/Hellraiser_Vol_1_17   \n",
       "0   /wiki/Ultimate_Spider-Man_Infinite_Comic_Vol_2_10   \n",
       "0   /wiki/Marvel_Universe_Ultimate_Spider-Man_vs._...   \n",
       "\n",
       "                                      Good characters  \\\n",
       "0                                                       \n",
       "0                                                       \n",
       "0   , /wiki/Human_Torch_(Android)_(Earth-616), /wi...   \n",
       "0   , /wiki/Thomas_Halloway_(Earth-616), /wiki/Bet...   \n",
       "0   , /wiki/Namor_McKenzie_(Earth-616), /wiki/Thak...   \n",
       "..                                                ...   \n",
       "0   , /wiki/Peter_Parker_(Earth-616), /wiki/Ben_Re...   \n",
       "0   , /wiki/Peter_Parker_(Earth-616), /wiki/Ben_Re...   \n",
       "0                                                       \n",
       "0   , /wiki/Peter_Parker_(Earth-12041), /wiki/Pete...   \n",
       "0   , /wiki/Web-Warriors_(Earth-12041), /wiki/Pete...   \n",
       "\n",
       "                                       Bad characters  \\\n",
       "0                                                       \n",
       "0                                                       \n",
       "0   , /wiki/Roglo_(Earth-616), #cite_note-Only_App...   \n",
       "0                     , /wiki/Emma_Martin_(Earth-616)   \n",
       "0                                                       \n",
       "..                                                ...   \n",
       "0   , /wiki/Kaine_Parker_(Earth-616), /wiki/Samuel...   \n",
       "0   , /wiki/Miles_Warren_(Jackal_Clone_2)_(Earth-616)   \n",
       "0                                                       \n",
       "0                       , /wiki/Shazana_(Earth-12041)   \n",
       "0   , /wiki/Norman_Osborn_(Earth-TRN457), /wiki/Hy...   \n",
       "\n",
       "                                   Neutral characters Editor-in-chief  \\\n",
       "0                                                                       \n",
       "0                                                          , Stan Lee   \n",
       "0   , /wiki/New_York_City_Police_Department_(Earth...     , Joe Simon   \n",
       "0                    , /wiki/Henry_Martin_(Earth-616)     , Joe Simon   \n",
       "0   , /wiki/Homo_mermanus, /wiki/New_York_City_Pol...     , Joe Simon   \n",
       "..                                                ...             ...   \n",
       "0   , /wiki/Guardian_(Spider-Clone)_(Earth-616), /...   , Joe Quesada   \n",
       "0   , /wiki/Kaine_Parker_(Earth-616), /wiki/Charle...   , Joe Quesada   \n",
       "0                                                                       \n",
       "0   , /wiki/William_Howard_Taft_(Earth-12041), /wi...   , Axel Alonso   \n",
       "0   , /wiki/Rio_Morales_(Earth-TRN457), /wiki/Jeff...   , Axel Alonso   \n",
       "\n",
       "    Editor-in-chief URL                      Writer  \\\n",
       "0                                   , Joe Caramagna   \n",
       "0      , /wiki/Stan_Lee                               \n",
       "0     , /wiki/Joe_Simon    , Stan Lee, Larry Lieber   \n",
       "0     , /wiki/Joe_Simon  , Paul Gustavson, Ray Gill   \n",
       "0     , /wiki/Joe_Simon     , William Blake Everett   \n",
       "..                  ...                         ...   \n",
       "0   , /wiki/Joe_Quesada            , J.M. DeMatteis   \n",
       "0   , /wiki/Joe_Quesada             , Howard Mackie   \n",
       "0                                    , Clive Barker   \n",
       "0   , /wiki/Axel_Alonso               , John Barber   \n",
       "0   , /wiki/Axel_Alonso             , Joe Caramagna   \n",
       "\n",
       "                                Writer URL Publication date  \\\n",
       "0                    , /wiki/Joe_Caramagna    January, 1943   \n",
       "0                                            December, 1942   \n",
       "0     , /wiki/Stan_Lee, /wiki/Larry_Lieber        May, 1940   \n",
       "0   , /wiki/Paul_Gustavson, /wiki/Ray_Gill        May, 1940   \n",
       "0            , /wiki/William_Blake_Everett        May, 1940   \n",
       "..                                     ...              ...   \n",
       "0                   , /wiki/J.M._DeMatteis             1979   \n",
       "0                    , /wiki/Howard_Mackie             1979   \n",
       "0                     , /wiki/Clive_Barker             1992   \n",
       "0                      , /wiki/John_Barber             2016   \n",
       "0                    , /wiki/Joe_Caramagna             2016   \n",
       "\n",
       "                         Subcomic  \n",
       "0                         st stor  \n",
       "0                          Morphy  \n",
       "0                 The Human Torch  \n",
       "0        The Angel: Master of Men  \n",
       "0   Prince Namor, the Sub-Mariner  \n",
       "..                            ...  \n",
       "0                   Resurrection!  \n",
       "0             Truths & Deceptions  \n",
       "0                    Resurrection  \n",
       "0              Ham-ilton (Part 2)  \n",
       "0                         st stor  \n",
       "\n",
       "[68482 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/comics_marvel.txt', 'rb') as f:\n",
    "    comics_marvel = pickle.load(f)\n",
    "\n",
    "comics_marvel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: DC dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's note that the DC dataset is extremely similar to the Marvel dataset. Indeed, the characters have the same characteristics and we can thus use the same attributes. The difference is that the characters are in two different pages which are classified into good and bad characters (compared to the Marvel dataset where we took all the characters in `Earth-616`, not knowing if they were good or bad). Moreover, the comics are also very similarly written and we can apply a similar code as the one we did for Marvel. We just have to be careful when we use the different `find` functions. Indeed, even if the characteristics are the same, the html code might be different and we had to adapt the code we had.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Parse the DC characters:\n",
    "\n",
    "### 2.1.1 Good DC characters\n",
    "\n",
    "**Collect the first URLs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://dc.fandom.com/wiki/Category:Good_Characters'\n",
    "r = requests.get(URL)\n",
    "page_body = r.text\n",
    "soup = BeautifulSoup(page_body, 'html.parser')\n",
    "\n",
    "# Collect URL of each character in the web page\n",
    "publications_wrappers = soup.find_all('li', class_='category-page__member')\n",
    "\n",
    "list_url = []\n",
    "for p in publications_wrappers:\n",
    "    for a in p.find_all('a', href=True):\n",
    "        list_url.append(a['href'])\n",
    "        \n",
    "# Remove duplicate\n",
    "my_set = set(list_url)\n",
    "good_list_url = list(my_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parse the first pages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = ['URL', 'Real Name', 'Current Alias', 'Relatives', 'Affiliation']\n",
    "\n",
    "personnage_pd = pd.DataFrame(columns=new_columns)\n",
    "idx = 0\n",
    "dict_geant = {}\n",
    "for pers in good_list_url:\n",
    "    # Get URL and use html parser\n",
    "    URL_char = 'https://dc.fandom.com/' + pers\n",
    "    URL_char = URL_char.replace(\"'\",\"\")\n",
    "    r_char = requests.get(URL_char)\n",
    "    page_body_char = r_char.text\n",
    "    soup_char = BeautifulSoup(page_body_char, 'html.parser')\n",
    "    \n",
    "    # Initialize variables\n",
    "    url, name, current_alias, relatives, affiliation = '','','','',''\n",
    "    personnage =[]\n",
    "    \n",
    "    # Parsing\n",
    "    url = pers\n",
    "    side_tab = soup_char.find_all('div', class_='mw-content-ltr mw-content-text')\n",
    "    for p in side_tab:\n",
    "        for pp in p.find_all('div', class_='pi-item pi-data pi-item-spacing pi-border-color'):\n",
    "            for ppp in pp.find_all('h3', class_='pi-data-label pi-secondary-font'):\n",
    "                if(ppp.text[0:]==\"Real Name\"):\n",
    "                    name = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Current Alias\"):\n",
    "                    current_alias = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Relatives\"):\n",
    "                    div = pp.find('div', class_='pi-data-value pi-font')\n",
    "                    if div:\n",
    "                        for a in div.find_all('a', href=True):\n",
    "                            relatives = relatives + ', ' + a['href']\n",
    "                if(ppp.text[0:]==\"Affiliation\"):\n",
    "                    div = pp.find('div', class_='pi-data-value pi-font')\n",
    "                    if div:\n",
    "                        for a in div.find_all('a', href=True):\n",
    "                            affiliation = affiliation + ', ' + a['href']\n",
    "\n",
    "    characteristics_pd = pd.DataFrame([[url, name, current_alias, relatives, affiliation]], columns = new_columns)\n",
    "\n",
    "    personnage_pd = personnage_pd.append(characteristics_pd,ignore_index=True)\n",
    "    \n",
    "nextpage = soup.find('link', {\"rel\" : \"next\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parse the remaining pages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101.339746 %"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-0995ab4260e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtot_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m11644\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnextpage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Print a loading bar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "tot_page = 11644/200\n",
    "\n",
    "while(len(nextpage['href'])):\n",
    "    # Print a loading bar\n",
    "    i += 1\n",
    "    printed= i/tot_page*100\n",
    "    stdout.write(\"\\r%f %%\" % printed)\n",
    "    stdout.flush()\n",
    "    \n",
    "    urlnext_page = nextpage['href']\n",
    "    r = requests.get(urlnext_page)\n",
    "    page_body = r.text\n",
    "    soup = BeautifulSoup(page_body, 'html.parser')\n",
    "\n",
    "    # Collect URL of each character in the web page\n",
    "    publications_wrappers = soup.find_all('li', class_='category-page__member')\n",
    "    list_url = []\n",
    "    for p in publications_wrappers:\n",
    "        for a in p.find_all('a', href=True):\n",
    "            list_url.append(a['href'])\n",
    "            \n",
    "    # Remove duplicate\n",
    "    my_set =set(list_url)\n",
    "    good_list_url = list(my_set)\n",
    "    idx =0\n",
    "    dict_geant={}\n",
    "    for pers in good_list_url:\n",
    "        # Get URL and use html parser\n",
    "        URL_char = 'https://dc.fandom.com/' + pers\n",
    "        URL_char = URL_char.replace(\"'\",\"\")\n",
    "        r_char = requests.get(URL_char)\n",
    "        page_body_char = r_char.text\n",
    "        soup_char = BeautifulSoup(page_body_char, 'html.parser')\n",
    "        \n",
    "        # Initialize variables\n",
    "        url, name, current_alias, relatives, affiliation = '','','','',''\n",
    "\n",
    "        personnage =[]\n",
    "\n",
    "        # Parsing\n",
    "        url = pers\n",
    "        side_tab = soup_char.find_all('div', class_='mw-content-ltr mw-content-text')\n",
    "        for p in side_tab:\n",
    "            for pp in p.find_all('div', class_='pi-item pi-data pi-item-spacing pi-border-color'):\n",
    "                for ppp in pp.find_all('h3', class_='pi-data-label pi-secondary-font'):\n",
    "                    if(ppp.text[0:]==\"Real Name\"):\n",
    "                        name = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                    if(ppp.text[0:]==\"Current Alias\"):\n",
    "                        current_alias = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                    if(ppp.text[0:]==\"Relatives\"):\n",
    "                        div = pp.find('div', class_='pi-data-value pi-font')\n",
    "                        if div:\n",
    "                            for a in div.find_all('a', href=True):\n",
    "                                relatives = relatives + ', ' + a['href']\n",
    "                    if(ppp.text[0:]==\"Affiliation\"):\n",
    "                        div = pp.find('div', class_='pi-data-value pi-font')\n",
    "                        if div:\n",
    "                            for a in div.find_all('a', href=True):\n",
    "                                affiliation = affiliation + ', ' + a['href']\n",
    "\n",
    "        characteristics_pd = pd.DataFrame([[url, name, current_alias, relatives, affiliation]], columns = new_columns)\n",
    "\n",
    "        personnage_pd = personnage_pd.append(characteristics_pd,ignore_index=True)\n",
    "    \n",
    "    nextpage = soup.find('link', {\"rel\" : \"next\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Bad DC characters\n",
    "\n",
    "**Collect the first URLs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://dc.fandom.com/wiki/Category:Bad_Characters'\n",
    "r = requests.get(URL)\n",
    "page_body = r.text\n",
    "soup = BeautifulSoup(page_body, 'html.parser')\n",
    "\n",
    "# Collect URL of each character in the web page\n",
    "publications_wrappers = soup.find_all('li', class_='category-page__member')\n",
    "\n",
    "list_url = []\n",
    "for p in publications_wrappers:\n",
    "    for a in p.find_all('a', href=True):\n",
    "        list_url.append(a['href'])\n",
    "        \n",
    "# Remove duplicate\n",
    "my_set = set(list_url)\n",
    "good_list_url = list(my_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parse the first pages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = ['URL', 'Real Name', 'Current Alias', 'Relatives', 'Affiliation']\n",
    "\n",
    "personnage_pd = pd.DataFrame(columns=new_columns)\n",
    "idx = 0\n",
    "dict_geant = {}\n",
    "for pers in good_list_url:\n",
    "    # Get URL and use html parser\n",
    "    URL_char = 'https://dc.fandom.com/' + pers\n",
    "    URL_char = URL_char.replace(\"'\",\"\")\n",
    "    r_char = requests.get(URL_char)\n",
    "    page_body_char = r_char.text\n",
    "    soup_char = BeautifulSoup(page_body_char, 'html.parser')\n",
    "    \n",
    "    # Initialize variables\n",
    "    url, name, current_alias, relatives, affiliation = '','','','',''\n",
    "    personnage =[]\n",
    "    \n",
    "    # Parsing\n",
    "    url = pers\n",
    "    side_tab = soup_char.find_all('div', class_='mw-content-ltr mw-content-text')\n",
    "    for p in side_tab:\n",
    "        for pp in p.find_all('div', class_='pi-item pi-data pi-item-spacing pi-border-color'):\n",
    "            for ppp in pp.find_all('h3', class_='pi-data-label pi-secondary-font'):\n",
    "                if(ppp.text[0:]==\"Real Name\"):\n",
    "                    name = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Current Alias\"):\n",
    "                    current_alias = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Relatives\"):\n",
    "                    div = pp.find('div', class_='pi-data-value pi-font')\n",
    "                    if div:\n",
    "                        for a in div.find_all('a', href=True):\n",
    "                            relatives = relatives + ', ' + a['href']\n",
    "                if(ppp.text[0:]==\"Affiliation\"):\n",
    "                    div = pp.find('div', class_='pi-data-value pi-font')\n",
    "                    if div:\n",
    "                        for a in div.find_all('a', href=True):\n",
    "                            affiliation = affiliation + ', ' + a['href']\n",
    "\n",
    "    characteristics_pd = pd.DataFrame([[url, name, current_alias, relatives, affiliation]], columns = new_columns)\n",
    "\n",
    "    personnage_pd = personnage_pd.append(characteristics_pd,ignore_index=True)\n",
    "    \n",
    "nextpage = soup.find('link', {\"rel\" : \"next\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parse the remaining pages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101.246106 %"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-c401547c1fc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtot_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10272\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnextpage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Print a loading bar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "tot_page = 10272/200\n",
    "\n",
    "while(len(nextpage['href'])):\n",
    "    # Print a loading bar\n",
    "    i += 1\n",
    "    printed= i/tot_page*100\n",
    "    stdout.write(\"\\r%f %%\" % printed)\n",
    "    stdout.flush()\n",
    "    \n",
    "    urlnext_page = nextpage['href']\n",
    "    r = requests.get(urlnext_page)\n",
    "    page_body = r.text\n",
    "    soup = BeautifulSoup(page_body, 'html.parser')\n",
    "\n",
    "    # Collect URL of each character in the web page\n",
    "    publications_wrappers = soup.find_all('li', class_='category-page__member')\n",
    "    list_url = []\n",
    "    for p in publications_wrappers:\n",
    "        for a in p.find_all('a', href=True):\n",
    "            list_url.append(a['href'])\n",
    "            \n",
    "    # Remove duplicate\n",
    "    my_set =set(list_url)\n",
    "    good_list_url = list(my_set)\n",
    "    idx =0\n",
    "    dict_geant={}\n",
    "    for pers in good_list_url:\n",
    "        # Get URL and use html parser\n",
    "        URL_char = 'https://dc.fandom.com/' + pers\n",
    "        URL_char = URL_char.replace(\"'\",\"\")\n",
    "        r_char = requests.get(URL_char)\n",
    "        page_body_char = r_char.text\n",
    "        soup_char = BeautifulSoup(page_body_char, 'html.parser')\n",
    "        \n",
    "        # Initialize variables\n",
    "        url, name, current_alias, relatives, affiliation = '','','','',''\n",
    "\n",
    "        personnage =[]\n",
    "\n",
    "        # Parsing\n",
    "        url = pers\n",
    "        side_tab = soup_char.find_all('div', class_='mw-content-ltr mw-content-text')\n",
    "        for p in side_tab:\n",
    "            for pp in p.find_all('div', class_='pi-item pi-data pi-item-spacing pi-border-color'):\n",
    "                for ppp in pp.find_all('h3', class_='pi-data-label pi-secondary-font'):\n",
    "                    if(ppp.text[0:]==\"Real Name\"):\n",
    "                        name = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                    if(ppp.text[0:]==\"Current Alias\"):\n",
    "                        current_alias = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                    if(ppp.text[0:]==\"Relatives\"):\n",
    "                        div = pp.find('div', class_='pi-data-value pi-font')\n",
    "                        if div:\n",
    "                            for a in div.find_all('a', href=True):\n",
    "                                relatives = relatives + ', ' + a['href']\n",
    "                    if(ppp.text[0:]==\"Affiliation\"):\n",
    "                        div = pp.find('div', class_='pi-data-value pi-font')\n",
    "                        if div:\n",
    "                            for a in div.find_all('a', href=True):\n",
    "                                affiliation = affiliation + ', ' + a['href']\n",
    "\n",
    "        characteristics_pd = pd.DataFrame([[url, name, current_alias, relatives, affiliation]], columns = new_columns)\n",
    "\n",
    "        personnage_pd = personnage_pd.append(characteristics_pd,ignore_index=True)\n",
    "    \n",
    "    nextpage = soup.find('link', {\"rel\" : \"next\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(personnage_pd, open('data/bad_dc.txt','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Join dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concatenate the dataframes for good and bad characters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Real Name</th>\n",
       "      <th>Current Alias</th>\n",
       "      <th>Relatives</th>\n",
       "      <th>Affiliation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Albert_Rothstein_(Arrow:_Earth-2)</td>\n",
       "      <td>Albert Rothstein</td>\n",
       "      <td>Atom Smasher</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>/wiki/Ahk-Ton_(New_Earth)</td>\n",
       "      <td>Ahk-Ton</td>\n",
       "      <td>Metamorpho</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>/wiki/Adam_Bomb_(New_Earth)</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Adam Bomb</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>/wiki/Adolf_Hitler_(JSA:_The_Golden_Age)</td>\n",
       "      <td>Adolf Hitler</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>, /wiki/Nazi_Party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>/wiki/Agarushnawokliag_(Prime_Earth)</td>\n",
       "      <td>Agarushnawokliag</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10472</td>\n",
       "      <td>/wiki/Zeta_(Earth-One)</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Zeta</td>\n",
       "      <td></td>\n",
       "      <td>, /wiki/Pantheon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10473</td>\n",
       "      <td>/wiki/Zotan_(Earth-S)</td>\n",
       "      <td>Zotan</td>\n",
       "      <td>Zotan</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10474</td>\n",
       "      <td>/wiki/Zond_(Earth-One)</td>\n",
       "      <td>Zond</td>\n",
       "      <td>Zond the Sorcerer</td>\n",
       "      <td></td>\n",
       "      <td>, /wiki/Morgaine_le_Fey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10475</td>\n",
       "      <td>/wiki/Zora_Vi-Lar_(Earth-One)</td>\n",
       "      <td>Zora Vi-Lar</td>\n",
       "      <td>Black Flame</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10476</td>\n",
       "      <td>/wiki/Category:Zatanna_villains</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10477 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            URL         Real Name  \\\n",
       "0       /wiki/Albert_Rothstein_(Arrow:_Earth-2)  Albert Rothstein   \n",
       "1                     /wiki/Ahk-Ton_(New_Earth)           Ahk-Ton   \n",
       "2                   /wiki/Adam_Bomb_(New_Earth)           Unknown   \n",
       "3      /wiki/Adolf_Hitler_(JSA:_The_Golden_Age)      Adolf Hitler   \n",
       "4          /wiki/Agarushnawokliag_(Prime_Earth)  Agarushnawokliag   \n",
       "...                                         ...               ...   \n",
       "10472                    /wiki/Zeta_(Earth-One)           Unknown   \n",
       "10473                     /wiki/Zotan_(Earth-S)             Zotan   \n",
       "10474                    /wiki/Zond_(Earth-One)              Zond   \n",
       "10475             /wiki/Zora_Vi-Lar_(Earth-One)       Zora Vi-Lar   \n",
       "10476           /wiki/Category:Zatanna_villains                     \n",
       "\n",
       "           Current Alias Relatives              Affiliation  \n",
       "0           Atom Smasher                                     \n",
       "1             Metamorpho                                     \n",
       "2              Adam Bomb                                     \n",
       "3                                        , /wiki/Nazi_Party  \n",
       "4                                                            \n",
       "...                  ...       ...                      ...  \n",
       "10472               Zeta                   , /wiki/Pantheon  \n",
       "10473              Zotan                                     \n",
       "10474  Zond the Sorcerer            , /wiki/Morgaine_le_Fey  \n",
       "10475        Black Flame                                     \n",
       "10476                                                        \n",
       "\n",
       "[10477 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/bad_dc.txt', 'rb') as f:\n",
    "    bad_dc = pickle.load(f)\n",
    "\n",
    "bad_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Real Name</th>\n",
       "      <th>Current Alias</th>\n",
       "      <th>Relatives</th>\n",
       "      <th>Affiliation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Aaron_Hayley_(New_Earth)</td>\n",
       "      <td>Aaron Hayley</td>\n",
       "      <td>Swamp Thing</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>/wiki/Abigail_Fine_(Smallville)</td>\n",
       "      <td>Abigail Fine</td>\n",
       "      <td>Abigail Fine</td>\n",
       "      <td>, /wiki/Elise_Fine_(Smallville)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>/wiki/Adam_Strange_(JSA:_The_Golden_Age)</td>\n",
       "      <td>Adam Strange</td>\n",
       "      <td>Adam Strange</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>/wiki/Alan_Barnes_(New_Earth)</td>\n",
       "      <td>Alan Barnes</td>\n",
       "      <td>Brainstorm</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>/wiki/Alan_Scott_(Earth_2)</td>\n",
       "      <td>Alan Scott</td>\n",
       "      <td>Green Lantern</td>\n",
       "      <td>, /wiki/Sam_Zhao_(Earth_2)</td>\n",
       "      <td>, /wiki/Wonders_of_the_World, /wiki/The_Green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11841</td>\n",
       "      <td>/wiki/Zor_In-Ze_(DCAU)</td>\n",
       "      <td>Zor In-Ze</td>\n",
       "      <td>Zor In-Ze</td>\n",
       "      <td>, /wiki/Kala_Im-Re_(DCAU), /wiki/Kara_In-Ze_(D...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11842</td>\n",
       "      <td>/wiki/Zoe_Lawton_(DC_Legends)</td>\n",
       "      <td>Zoe Lawton</td>\n",
       "      <td></td>\n",
       "      <td>, /wiki/Floyd_Lawton_(DC_Legends)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11843</td>\n",
       "      <td>/wiki/Zilya_Popoff_(Earth-Prime)</td>\n",
       "      <td>Zilya Popoff</td>\n",
       "      <td>Zilya Popoff</td>\n",
       "      <td></td>\n",
       "      <td>, /wiki/United_Planets_Young_Heroes_(Earth-Prime)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11844</td>\n",
       "      <td>/wiki/Zor-El_(DC_Legends)</td>\n",
       "      <td>Zor-El</td>\n",
       "      <td></td>\n",
       "      <td>, /wiki/Jor-El_(DC_Legends), /wiki/Kara_Zor-El...</td>\n",
       "      <td>, /wiki/House_of_El</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11845</td>\n",
       "      <td>/wiki/Zor-El_(Arrow:_Earth-38)</td>\n",
       "      <td>Zor-El</td>\n",
       "      <td></td>\n",
       "      <td>, /wiki/Alura_In-Ze_(Arrow:_Earth-38), /wiki/K...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11846 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            URL     Real Name  Current Alias  \\\n",
       "0                /wiki/Aaron_Hayley_(New_Earth)  Aaron Hayley    Swamp Thing   \n",
       "1               /wiki/Abigail_Fine_(Smallville)  Abigail Fine   Abigail Fine   \n",
       "2      /wiki/Adam_Strange_(JSA:_The_Golden_Age)  Adam Strange   Adam Strange   \n",
       "3                 /wiki/Alan_Barnes_(New_Earth)   Alan Barnes     Brainstorm   \n",
       "4                    /wiki/Alan_Scott_(Earth_2)    Alan Scott  Green Lantern   \n",
       "...                                         ...           ...            ...   \n",
       "11841                    /wiki/Zor_In-Ze_(DCAU)     Zor In-Ze      Zor In-Ze   \n",
       "11842             /wiki/Zoe_Lawton_(DC_Legends)    Zoe Lawton                  \n",
       "11843          /wiki/Zilya_Popoff_(Earth-Prime)  Zilya Popoff   Zilya Popoff   \n",
       "11844                 /wiki/Zor-El_(DC_Legends)        Zor-El                  \n",
       "11845            /wiki/Zor-El_(Arrow:_Earth-38)        Zor-El                  \n",
       "\n",
       "                                               Relatives  \\\n",
       "0                                                          \n",
       "1                        , /wiki/Elise_Fine_(Smallville)   \n",
       "2                                                          \n",
       "3                                                          \n",
       "4                             , /wiki/Sam_Zhao_(Earth_2)   \n",
       "...                                                  ...   \n",
       "11841  , /wiki/Kala_Im-Re_(DCAU), /wiki/Kara_In-Ze_(D...   \n",
       "11842                  , /wiki/Floyd_Lawton_(DC_Legends)   \n",
       "11843                                                      \n",
       "11844  , /wiki/Jor-El_(DC_Legends), /wiki/Kara_Zor-El...   \n",
       "11845  , /wiki/Alura_In-Ze_(Arrow:_Earth-38), /wiki/K...   \n",
       "\n",
       "                                             Affiliation  \n",
       "0                                                         \n",
       "1                                                         \n",
       "2                                                         \n",
       "3                                                         \n",
       "4          , /wiki/Wonders_of_the_World, /wiki/The_Green  \n",
       "...                                                  ...  \n",
       "11841                                                     \n",
       "11842                                                     \n",
       "11843  , /wiki/United_Planets_Young_Heroes_(Earth-Prime)  \n",
       "11844                                , /wiki/House_of_El  \n",
       "11845                                                     \n",
       "\n",
       "[11846 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/good_dc.txt', 'rb') as f:\n",
    "    good_dc = pickle.load(f)\n",
    "\n",
    "good_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers_dc = [good_dc, bad_dc]\n",
    "characters_dc = pd.concat(pers_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save to pickle:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(characters_dc, open('data/characters_dc.txt','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open the saved file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Real Name</th>\n",
       "      <th>Current Alias</th>\n",
       "      <th>Relatives</th>\n",
       "      <th>Affiliation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Aaron_Hayley_(New_Earth)</td>\n",
       "      <td>Aaron Hayley</td>\n",
       "      <td>Swamp Thing</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>/wiki/Abigail_Fine_(Smallville)</td>\n",
       "      <td>Abigail Fine</td>\n",
       "      <td>Abigail Fine</td>\n",
       "      <td>, /wiki/Elise_Fine_(Smallville)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>/wiki/Adam_Strange_(JSA:_The_Golden_Age)</td>\n",
       "      <td>Adam Strange</td>\n",
       "      <td>Adam Strange</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>/wiki/Alan_Barnes_(New_Earth)</td>\n",
       "      <td>Alan Barnes</td>\n",
       "      <td>Brainstorm</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>/wiki/Alan_Scott_(Earth_2)</td>\n",
       "      <td>Alan Scott</td>\n",
       "      <td>Green Lantern</td>\n",
       "      <td>, /wiki/Sam_Zhao_(Earth_2)</td>\n",
       "      <td>, /wiki/Wonders_of_the_World, /wiki/The_Green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10472</td>\n",
       "      <td>/wiki/Zeta_(Earth-One)</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Zeta</td>\n",
       "      <td></td>\n",
       "      <td>, /wiki/Pantheon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10473</td>\n",
       "      <td>/wiki/Zotan_(Earth-S)</td>\n",
       "      <td>Zotan</td>\n",
       "      <td>Zotan</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10474</td>\n",
       "      <td>/wiki/Zond_(Earth-One)</td>\n",
       "      <td>Zond</td>\n",
       "      <td>Zond the Sorcerer</td>\n",
       "      <td></td>\n",
       "      <td>, /wiki/Morgaine_le_Fey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10475</td>\n",
       "      <td>/wiki/Zora_Vi-Lar_(Earth-One)</td>\n",
       "      <td>Zora Vi-Lar</td>\n",
       "      <td>Black Flame</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10476</td>\n",
       "      <td>/wiki/Category:Zatanna_villains</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22323 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            URL     Real Name  \\\n",
       "0                /wiki/Aaron_Hayley_(New_Earth)  Aaron Hayley   \n",
       "1               /wiki/Abigail_Fine_(Smallville)  Abigail Fine   \n",
       "2      /wiki/Adam_Strange_(JSA:_The_Golden_Age)  Adam Strange   \n",
       "3                 /wiki/Alan_Barnes_(New_Earth)   Alan Barnes   \n",
       "4                    /wiki/Alan_Scott_(Earth_2)    Alan Scott   \n",
       "...                                         ...           ...   \n",
       "10472                    /wiki/Zeta_(Earth-One)       Unknown   \n",
       "10473                     /wiki/Zotan_(Earth-S)         Zotan   \n",
       "10474                    /wiki/Zond_(Earth-One)          Zond   \n",
       "10475             /wiki/Zora_Vi-Lar_(Earth-One)   Zora Vi-Lar   \n",
       "10476           /wiki/Category:Zatanna_villains                 \n",
       "\n",
       "           Current Alias                        Relatives  \\\n",
       "0            Swamp Thing                                    \n",
       "1           Abigail Fine  , /wiki/Elise_Fine_(Smallville)   \n",
       "2           Adam Strange                                    \n",
       "3             Brainstorm                                    \n",
       "4          Green Lantern       , /wiki/Sam_Zhao_(Earth_2)   \n",
       "...                  ...                              ...   \n",
       "10472               Zeta                                    \n",
       "10473              Zotan                                    \n",
       "10474  Zond the Sorcerer                                    \n",
       "10475        Black Flame                                    \n",
       "10476                                                       \n",
       "\n",
       "                                         Affiliation  \n",
       "0                                                     \n",
       "1                                                     \n",
       "2                                                     \n",
       "3                                                     \n",
       "4      , /wiki/Wonders_of_the_World, /wiki/The_Green  \n",
       "...                                              ...  \n",
       "10472                               , /wiki/Pantheon  \n",
       "10473                                                 \n",
       "10474                        , /wiki/Morgaine_le_Fey  \n",
       "10475                                                 \n",
       "10476                                                 \n",
       "\n",
       "[22323 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_dc = pd.read_pickle('data/characters_dc.txt')\n",
    "\n",
    "character_dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Parse the DC comics:\n",
    "\n",
    "**Collect the first URLs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get URL and use html parser\n",
    "URL = 'https://dc.fandom.com/wiki/Category:Comics'\n",
    "r = requests.get(URL)\n",
    "page_body = r.text\n",
    "soup = BeautifulSoup(page_body, 'html.parser')\n",
    "\n",
    "# Collect URL of each character in the web page\n",
    "publications_wrappers = soup.find_all('li', class_='category-page__member')\n",
    "\n",
    "# Initialization\n",
    "i = 0\n",
    "list_url = []\n",
    "\n",
    "for p in publications_wrappers:\n",
    "    # Don't take into account the first links which are \"Categories\"\n",
    "    if(i>=24):\n",
    "        for a in p.find_all('a', href=True):\n",
    "            list_url.append(a['href'])\n",
    "    i +=1\n",
    "    \n",
    "# Remove duplicate\n",
    "my_set = set(list_url)\n",
    "good_list_url = list(my_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parse the first pages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comics_columns = ['URL', 'Good characters', 'Bad characters', \n",
    "                  'Neutral characters', 'Editor-in-chief', 'Editor-in-chief URL', \n",
    "                  'Writer', 'Writer URL', 'Publication date', 'Subcomic']\n",
    "\n",
    "comics_pd = pd.DataFrame(columns=comics_columns)\n",
    "\n",
    "# Parse the first page\n",
    "for comics in good_list_url:\n",
    "    # Get URL and use html parser\n",
    "    URL_char = 'https://dc.fandom.com' + comics\n",
    "    URL_char = URL_char.replace(\"'\",\"\")\n",
    "    r_char = requests.get(URL_char)\n",
    "    page_body_char = r_char.text\n",
    "    soup_char = BeautifulSoup(page_body_char, 'html.parser')\n",
    "    \n",
    "    # Initialization\n",
    "    good, bad, neutral = '','',''\n",
    "    editor, writer, publication = '','',''\n",
    "    editorURL, writerURL, subcomic = '','',''\n",
    "    URL2 = URL_char.replace('https://dc.fandom.com','')\n",
    "    \n",
    "    # Boolean for the first case\n",
    "    first = 1\n",
    "    \n",
    "    # Parse characters appearances\n",
    "    appearances = soup_char.find_all('div', class_='mw-content-ltr mw-content-text')\n",
    "    for p in appearances:\n",
    "        for pp in p.find_all('p'):\n",
    "            # If a comic is split in sub-comics\n",
    "            span = pp.find_previous('span', class_='mw-headline')\n",
    "            if span and (\"Appearing\" in span.text):\n",
    "                if (not first and subcomic!=span.text \\\n",
    "                and (good or bad or neutral)):\n",
    "                    # Take only the title of the subcomic\n",
    "                    subcomic = subcomic[14:-1]\n",
    "                    appearances_pd = pd.DataFrame([[URL2, good, bad, neutral, \n",
    "                                        editor, editorURL,\n",
    "                                        writer, writerURL,\n",
    "                                        publication, subcomic]], columns = comics_columns)\n",
    "                    comics_pd = comics_pd.append(appearances_pd)\n",
    "                    good, bad, neutral = '','',''\n",
    "                    editor, writer, publication = '','',''\n",
    "                    editorURL, writerURL = '',''\n",
    "                    subcomic = span.text\n",
    "                else:\n",
    "                    first = 0\n",
    "                    subcomic = span.text\n",
    "                    \n",
    "            if \"Featured Characters:\" in pp.text:\n",
    "                ul = pp.find_next('ul')\n",
    "                for a in ul.find_all('a', href=True):\n",
    "                    good = good + ', ' + a['href']\n",
    "            if \"Supporting Characters:\" in pp.text:\n",
    "                ul = pp.find_next('ul')\n",
    "                for a in ul.find_all('a', href=True):\n",
    "                    good = good + ', ' + a['href']\n",
    "            if \"Antagonists:\" in pp.text:\n",
    "                ul = pp.find_next('ul')\n",
    "                for a in ul.find_all('a', href=True):\n",
    "                    bad = bad + ', ' + a['href']\n",
    "            if \"Other Characters:\" in pp.text:\n",
    "                ul = pp.find_next('ul')\n",
    "                for a in ul.find_all('a', href=True):\n",
    "                    neutral = neutral + ', ' + a['href']\n",
    "                    \n",
    "    # Each subcomic is written in the format '\"Appearing in *subcomic*\"'\n",
    "    # We keep only the subcomic name\n",
    "    subcomic = subcomic[14:-1]\n",
    "    appearances_pd = pd.DataFrame([[URL2, good, bad, neutral, \n",
    "                                    editor, editorURL,\n",
    "                                    writer, writerURL,\n",
    "                                    publication, subcomic]], columns = comics_columns)\n",
    "    comics_pd = comics_pd.append(appearances_pd)\n",
    "    subcomic = ''\n",
    "    \n",
    "    \n",
    "    # Parse comic characteristics\n",
    "    side_tab = soup_char.find_all('div', class_='mw-content-ltr mw-content-text')\n",
    "    for p in side_tab:\n",
    "        # Publication date\n",
    "        if(p.find_next('h2', {'data-source':'StoryTitle1'})):\n",
    "            tab = p.find_next('h2', class_='pi-item pi-item-spacing pi-title')\n",
    "        if(tab):\n",
    "            tab = tab.find_next('h2', class_='pi-item pi-item-spacing pi-title')\n",
    "        if(tab):\n",
    "            tab = tab.find_next('h2', class_='pi-item pi-item-spacing pi-title')\n",
    "        if(tab):\n",
    "            publication = tab.text\n",
    "            \n",
    "        # Editors\n",
    "        for pp in p.find_all('h3', class_='pi-data-label pi-secondary-font'):\n",
    "            if ('Executive Editor' in pp.text):\n",
    "                ppp = pp.find_next('div', class_='pi-data-value pi-font')\n",
    "                for a in ppp.find_all('a'):\n",
    "                    editor = editor + ', ' + a.text\n",
    "                for a in ppp.find_all('a', href=True):\n",
    "                    editorURL = editorURL + ', ' + a['href']\n",
    "\n",
    "        sub = tab.find_next('h2', class_='pi-item pi-header pi-secondary-font pi-item-spacing pi-secondary-background')\n",
    "\n",
    "        # Writers\n",
    "        for pp in p.find_all('h3', class_='pi-data-label pi-secondary-font'):\n",
    "            if(pp.find_previous('h2', class_='pi-item pi-header pi-secondary-font pi-item-spacing pi-secondary-background')):\n",
    "                sub = pp.find_previous('h2', class_='pi-item pi-header pi-secondary-font pi-item-spacing pi-secondary-background')\n",
    "            if sub:\n",
    "                adiv = sub.find_next('h3', class_='pi-data-label pi-secondary-font')\n",
    "            if (adiv and 'Writer' in adiv.text and 'Writer' in pp.text):\n",
    "                if (sub):\n",
    "                    subcomic = sub.text[1:-1]\n",
    "                ppp = pp.find_next('div', class_='pi-data-value pi-font')\n",
    "                for a in ppp.find_all('a'):\n",
    "                    writer = writer + ', ' + a.text\n",
    "                for a in ppp.find_all('a', href=True):\n",
    "                    writerURL = writerURL + ', ' + a['href']\n",
    "                comics_pd.loc[comics_pd.Subcomic == subcomic,'Writer'] = writer\n",
    "                comics_pd.loc[comics_pd.Subcomic == subcomic,'Writer URL'] = writerURL\n",
    "                if (sub):\n",
    "                    sub = sub.find_next('h2', class_='pi-item pi-header pi-secondary-font pi-item-spacing pi-secondary-background')\n",
    "                    writer, writerURL = '',''\n",
    "            elif (sub): \n",
    "                sub = sub.find_next('h2', class_='pi-item pi-header pi-secondary-font pi-item-spacing pi-secondary-background')\n",
    "                writer, writerURL = '',''    \n",
    "                \n",
    "    comics_pd.loc[comics_pd.URL == URL2,'Editor-in-chief'] = editor\n",
    "    comics_pd.loc[comics_pd.URL == URL2,'Editor-in-chief URL'] = editorURL\n",
    "    comics_pd.loc[comics_pd.URL == URL2,'Publication date'] = publication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parse the remaining pages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all the other pages\n",
    "nextpage = soup.find('link', {\"rel\" : \"next\"})\n",
    "\n",
    "i = 0\n",
    "tot_page = 49759/200\n",
    "\n",
    "while(len(nextpage['href'])):\n",
    "    # Print a loading bar\n",
    "    i += 1\n",
    "    printed= i/tot_page*100\n",
    "    stdout.write(\"\\r%f %%\" % printed)\n",
    "    stdout.flush()\n",
    "    urlnext_page = nextpage['href']\n",
    "    r = requests.get(urlnext_page)\n",
    "    page_body = r.text\n",
    "    soup = BeautifulSoup(page_body, 'html.parser')\n",
    "    \n",
    "    # Collect url of each character in the web page\n",
    "    publications_wrappers = soup.find_all('li', class_='category-page__member')\n",
    "    list_url = []\n",
    "    for p in publications_wrappers:\n",
    "        for a in p.find_all('a', href=True):\n",
    "            list_url.append(a['href'])\n",
    "          \n",
    "    # Remove duplicates\n",
    "    my_set = set(list_url)\n",
    "    good_list_url = list(my_set)\n",
    "    good_list_url = [ x for x in good_list_url if \"/wiki/Category:\" not in x ]\n",
    "  \n",
    "    # Parse the first page\n",
    "    for comics in good_list_url:\n",
    "        # Get URL and use html parser\n",
    "        URL_char = 'https://dc.fandom.com' + comics\n",
    "        URL_char = URL_char.replace(\"'\",\"\")\n",
    "        r_char = requests.get(URL_char)\n",
    "        page_body_char = r_char.text\n",
    "        soup_char = BeautifulSoup(page_body_char, 'html.parser')\n",
    "        \n",
    "        # Initialization\n",
    "        good, bad, neutral = '','',''\n",
    "        editor, writer, publication = '','',''\n",
    "        editorURL, writerURL, subcomic = '','',''\n",
    "        URL2 = URL_char.replace('https://dc.fandom.com','')\n",
    "\n",
    "        # Boolean for the first case\n",
    "        first = 1\n",
    "\n",
    "        # Parse characters appearances\n",
    "        appearances = soup_char.find_all('div', class_='mw-content-ltr mw-content-text')\n",
    "        for p in appearances:\n",
    "            for pp in p.find_all('p'):\n",
    "                # If a comic is split in sub-comics\n",
    "                span = pp.find_previous('span', class_='mw-headline')\n",
    "                if span and (\"Appearing\" in span.text):\n",
    "                    if (not first and subcomic!=span.text \\\n",
    "                    and (good or bad or neutral)):\n",
    "                        # Take only the title of the subcomic\n",
    "                        subcomic = subcomic[14:-1]\n",
    "                        appearances_pd = pd.DataFrame([[URL2, good, bad, neutral, \n",
    "                                              editor, editorURL,\n",
    "                                              writer, writerURL,\n",
    "                                              publication, subcomic]], columns = comics_columns)\n",
    "                        comics_pd = comics_pd.append(appearances_pd)\n",
    "\n",
    "                        good, bad, neutral = '','',''\n",
    "                        editor, writer, publication = '','',''\n",
    "                        editorURL, writerURL = '',''\n",
    "                        subcomic = span.text\n",
    "                    else:\n",
    "                        first = 0\n",
    "                        subcomic = span.text\n",
    "                if \"Featured Characters:\" in pp.text:\n",
    "                    ul = pp.find_next('ul')\n",
    "                    for a in ul.find_all('a', href=True):\n",
    "                        good = good + ', ' + a['href']\n",
    "                if \"Supporting Characters:\" in pp.text:\n",
    "                    ul = pp.find_next('ul')\n",
    "                    for a in ul.find_all('a', href=True):\n",
    "                        good = good + ', ' + a['href']\n",
    "                if \"Antagonists:\" in pp.text:\n",
    "                    ul = pp.find_next('ul')\n",
    "                    for a in ul.find_all('a', href=True):\n",
    "                        bad = bad + ', ' + a['href']\n",
    "                if \"Other Characters:\" in pp.text:\n",
    "                    ul = pp.find_next('ul')\n",
    "                    for a in ul.find_all('a', href=True):\n",
    "                        neutral = neutral + ', ' + a['href']\n",
    "\n",
    "        # Each subcomic is written in the format '\"Appearing in *subcomic*\"'\n",
    "        # We keep only the subcomic name\n",
    "        subcomic = subcomic[14:-1]\n",
    "\n",
    "        appearances_pd = pd.DataFrame([[URL2, good, bad, neutral, \n",
    "                                          editor, editorURL,\n",
    "                                          writer, writerURL,\n",
    "                                          publication, subcomic]], columns = comics_columns)\n",
    "\n",
    "        comics_pd = comics_pd.append(appearances_pd)\n",
    "\n",
    "        subcomic = ''\n",
    "\n",
    "\n",
    "        # Parse comic characteristics\n",
    "        side_tab = soup_char.find_all('div', class_='mw-content-ltr mw-content-text')\n",
    "        for p in side_tab:\n",
    "            # Publication date\n",
    "            if(p.find_next('h2', {'data-source':'StoryTitle1'})):\n",
    "                tab = p.find_next('h2', class_='pi-item pi-item-spacing pi-title')\n",
    "                if(tab):\n",
    "                    tab = tab.find_next('h2', class_='pi-item pi-item-spacing pi-title')\n",
    "                if(tab):\n",
    "                    tab = tab.find_next('h2', class_='pi-item pi-item-spacing pi-title')\n",
    "                if(tab):\n",
    "                    publication = tab.text\n",
    "            else:\n",
    "                if(tab):\n",
    "                    tab = p.find_next('h2', class_='pi-item pi-item-spacing pi-title')\n",
    "                if(tab):\n",
    "                    tab = tab.find_next('h2', class_='pi-item pi-item-spacing pi-title')\n",
    "                if(tab):\n",
    "                    publication = tab.text\n",
    "\n",
    "            # Editors\n",
    "            for pp in p.find_all('h3', class_='pi-data-label pi-secondary-font'):\n",
    "                if ('Executive Editor' in pp.text):\n",
    "                    ppp = pp.find_next('div', class_='pi-data-value pi-font')\n",
    "                    for a in ppp.find_all('a'):\n",
    "                        editor = editor + ', ' + a.text\n",
    "                    for a in ppp.find_all('a', href=True):\n",
    "                        editorURL = editorURL + ', ' + a['href']\n",
    "            if(tab):           \n",
    "                if(tab.find_next('h2', class_='pi-item pi-header pi-secondary-font pi-item-spacing pi-secondary-background')):\n",
    "                      sub = tab.find_next('h2', class_='pi-item pi-header pi-secondary-font pi-item-spacing pi-secondary-background')\n",
    "\n",
    "            # Writers\n",
    "            for pp in p.find_all('h3', class_='pi-data-label pi-secondary-font'):\n",
    "                if(pp.find_previous('h2', class_='pi-item pi-header pi-secondary-font pi-item-spacing pi-secondary-background')):\n",
    "                    sub = pp.find_previous('h2', class_='pi-item pi-header pi-secondary-font pi-item-spacing pi-secondary-background')\n",
    "                if sub:\n",
    "                    adiv = sub.find_next('h3', class_='pi-data-label pi-secondary-font')\n",
    "                if (adiv and 'Writer' in adiv.text and 'Writer' in pp.text):\n",
    "                    if (sub):\n",
    "                        subcomic = sub.text[1:-1]\n",
    "                        ppp = pp.find_next('div', class_='pi-data-value pi-font')\n",
    "                    for a in ppp.find_all('a'):\n",
    "                        writer = writer + ', ' + a.text\n",
    "                    for a in ppp.find_all('a', href=True):\n",
    "                        writerURL = writerURL + ', ' + a['href']\n",
    "                    comics_pd.loc[comics_pd.Subcomic == subcomic,'Writer'] = writer\n",
    "                    comics_pd.loc[comics_pd.Subcomic == subcomic,'Writer URL'] = writerURL\n",
    "                    if (sub):\n",
    "                        sub = sub.find_next('h2', class_='pi-item pi-header pi-secondary-font pi-item-spacing pi-secondary-background')\n",
    "                        writer, writerURL = '',''\n",
    "                elif (sub): \n",
    "                    sub = sub.find_next('h2', class_='pi-item pi-header pi-secondary-font pi-item-spacing pi-secondary-background')\n",
    "                    writer, writerURL = '',''    \n",
    "                \n",
    "        comics_pd.loc[comics_pd.URL == URL2,'Editor-in-chief'] = editor\n",
    "        comics_pd.loc[comics_pd.URL == URL2,'Editor-in-chief URL'] = editorURL\n",
    "        comics_pd.loc[comics_pd.URL == URL2,'Publication date'] = publication\n",
    "    nextpage = soup.find('link', {\"rel\" : \"next\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save to pickle:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(comics_pd, open('data/comics_dc.txt','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open the saved file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Good characters</th>\n",
       "      <th>Bad characters</th>\n",
       "      <th>Neutral characters</th>\n",
       "      <th>Editor-in-chief</th>\n",
       "      <th>Editor-in-chief URL</th>\n",
       "      <th>Writer</th>\n",
       "      <th>Writer URL</th>\n",
       "      <th>Publication date</th>\n",
       "      <th>Subcomic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/100_Bullets_Vol_1_64</td>\n",
       "      <td>, /wiki/Jack_Daw_(100_Bullets), /wiki/Philip_G...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>, Karen Berger</td>\n",
       "      <td>, /wiki/Karen_Berger</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>November, 2005</td>\n",
       "      <td>The Dive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/100_Bullets_Vol_1_25</td>\n",
       "      <td>, /wiki/Augustus_Medici_(100_Bullets), /wiki/B...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>, Karen Berger</td>\n",
       "      <td>, /wiki/Karen_Berger</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>August, 2001</td>\n",
       "      <td>Red Prince Blues (Part III of III)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/2020_Visions_Vol_1_5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>, Karen Berger</td>\n",
       "      <td>, /wiki/Karen_Berger</td>\n",
       "      <td>, Ron Marz</td>\n",
       "      <td>, /wiki/Ron_Marz</td>\n",
       "      <td>September, 1997</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/100%25_True%3F_Vol_1_2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>, Jenette Kahn</td>\n",
       "      <td>, /wiki/Jenette_Kahn</td>\n",
       "      <td>, Ron Marz</td>\n",
       "      <td>, /wiki/Ron_Marz</td>\n",
       "      <td>December, 1997</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/100_Bullets_Vol_1_11</td>\n",
       "      <td>, /wiki/Philip_Graves_(100_Bullets)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>, Karen Berger</td>\n",
       "      <td>, /wiki/Karen_Berger</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>June, 2000</td>\n",
       "      <td>Heartbreak, Sunny Side Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Zatanna_Vol_2_1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>, Dan DiDio</td>\n",
       "      <td>, /wiki/Dan_DiDio</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>July, 2010</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Zero_Girl_Vol_1_4</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>, Jim Lee</td>\n",
       "      <td>, /wiki/Jim_Lee</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>May, 2001</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Young_Romance_Vol_1_196</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>December, 1973</td>\n",
       "      <td>he 1st Stor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Young_Romance_Vol_1_126</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>November, 1963</td>\n",
       "      <td>he 1st Stor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Young_Romance_Vol_1_200</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>August, 1974</td>\n",
       "      <td>he 1st Stor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62314 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              URL  \\\n",
       "0      /wiki/100_Bullets_Vol_1_64   \n",
       "0      /wiki/100_Bullets_Vol_1_25   \n",
       "0      /wiki/2020_Visions_Vol_1_5   \n",
       "0    /wiki/100%25_True%3F_Vol_1_2   \n",
       "0      /wiki/100_Bullets_Vol_1_11   \n",
       "..                            ...   \n",
       "0           /wiki/Zatanna_Vol_2_1   \n",
       "0         /wiki/Zero_Girl_Vol_1_4   \n",
       "0   /wiki/Young_Romance_Vol_1_196   \n",
       "0   /wiki/Young_Romance_Vol_1_126   \n",
       "0   /wiki/Young_Romance_Vol_1_200   \n",
       "\n",
       "                                      Good characters Bad characters  \\\n",
       "0   , /wiki/Jack_Daw_(100_Bullets), /wiki/Philip_G...                  \n",
       "0   , /wiki/Augustus_Medici_(100_Bullets), /wiki/B...                  \n",
       "0                                                                      \n",
       "0                                                                      \n",
       "0                 , /wiki/Philip_Graves_(100_Bullets)                  \n",
       "..                                                ...            ...   \n",
       "0                                                                      \n",
       "0                                                                      \n",
       "0                                                                      \n",
       "0                                                                      \n",
       "0                                                                      \n",
       "\n",
       "   Neutral characters Editor-in-chief   Editor-in-chief URL      Writer  \\\n",
       "0                      , Karen Berger  , /wiki/Karen_Berger               \n",
       "0                      , Karen Berger  , /wiki/Karen_Berger               \n",
       "0                      , Karen Berger  , /wiki/Karen_Berger  , Ron Marz   \n",
       "0                      , Jenette Kahn  , /wiki/Jenette_Kahn  , Ron Marz   \n",
       "0                      , Karen Berger  , /wiki/Karen_Berger               \n",
       "..                ...             ...                   ...         ...   \n",
       "0                         , Dan DiDio     , /wiki/Dan_DiDio               \n",
       "0                           , Jim Lee       , /wiki/Jim_Lee               \n",
       "0                                                                         \n",
       "0                                                                         \n",
       "0                                                                         \n",
       "\n",
       "          Writer URL Publication date                            Subcomic  \n",
       "0                      November, 2005                            The Dive  \n",
       "0                        August, 2001  Red Prince Blues (Part III of III)  \n",
       "0   , /wiki/Ron_Marz  September, 1997                                      \n",
       "0   , /wiki/Ron_Marz   December, 1997                                      \n",
       "0                          June, 2000           Heartbreak, Sunny Side Up  \n",
       "..               ...              ...                                 ...  \n",
       "0                          July, 2010                                      \n",
       "0                           May, 2001                                      \n",
       "0                      December, 1973                         he 1st Stor  \n",
       "0                      November, 1963                         he 1st Stor  \n",
       "0                        August, 1974                         he 1st Stor  \n",
       "\n",
       "[62314 rows x 10 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/comics_dc.txt', 'rb') as f:\n",
    "    comics_pd = pickle.load(f)\n",
    "\n",
    "comics_pd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
